{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Assignment 1\n",
    "#### Machine Learning (CSCI-4930/5930)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the dataset into pandas dataframe\n",
    "training = pd.read_csv('dataset/training.csv',header=None,delimiter=',')\n",
    "validation = pd.read_csv('dataset/validation.csv',header=None,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-315</td>\n",
       "      <td>75</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-243</td>\n",
       "      <td>75</td>\n",
       "      <td>0.213333</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-289</td>\n",
       "      <td>77</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.220779</td>\n",
       "      <td>0.220779</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-776</td>\n",
       "      <td>118</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.203390</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-388</td>\n",
       "      <td>77</td>\n",
       "      <td>0.226190</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1         2         3         4         5         6         7  8\n",
       "0 -315   75  0.205128  0.230769  0.307692  0.146667  0.280000  0.240000  0\n",
       "1 -243   75  0.213333  0.186667  0.293333  0.400000  0.253333  0.186667  0\n",
       "2 -289   77  0.192308  0.230769  0.282051  0.220779  0.220779  0.272727  0\n",
       "3 -776  118  0.271186  0.203390  0.271186  0.200000  0.200000  0.291667  1\n",
       "4 -388   77  0.226190  0.238095  0.250000  0.272727  0.272727  0.207792  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : A\n",
    "Print total number of samples in the validation dataset: \"dataset/validation.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting entire size of dataset \n",
      "          0    1         2         3         4         5         6         7  8\n",
      "0     -315   75  0.205128  0.230769  0.307692  0.146667  0.280000  0.240000  0\n",
      "1     -243   75  0.213333  0.186667  0.293333  0.400000  0.253333  0.186667  0\n",
      "2     -289   77  0.192308  0.230769  0.282051  0.220779  0.220779  0.272727  0\n",
      "3     -776  118  0.271186  0.203390  0.271186  0.200000  0.200000  0.291667  1\n",
      "4     -388   77  0.226190  0.238095  0.250000  0.272727  0.272727  0.207792  1\n",
      "...    ...  ...       ...       ...       ...       ...       ...       ... ..\n",
      "59995 -707  120  0.233333  0.216667  0.266667  0.233333  0.241667  0.250000  1\n",
      "59996 -404  117  0.247863  0.239316  0.205128  0.226891  0.218487  0.252101  0\n",
      "59997 -847  120  0.233333  0.208333  0.275000  0.233333  0.208333  0.258333  1\n",
      "59998 -201   75  0.200000  0.226667  0.293333  0.207792  0.181818  0.298701  0\n",
      "59999 -770  120  0.237705  0.122951  0.295082  0.208333  0.216667  0.266667  1\n",
      "\n",
      "[60000 rows x 9 columns]\n",
      "Another way using shape (60000, 9)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(validation)\n",
    "print(\"Getting entire size of dataset \\n\", df)\n",
    "\n",
    "print(\"Another way using shape\",validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples = 60000\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of samples =\", len(validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : B\n",
    "Print two numbers in the format: [n0, n1], where\n",
    "n0 represents number of class=0 (negative) samples and n1 represents number of class=1 (positive) samples in the validation dataset: \"dataset/validation.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using iloc \n",
      "      0   1         2         3         4         5     6     7  8\n",
      "0 -315  75  0.205128  0.230769  0.307692  0.146667  0.28  0.24  0\n",
      "\n",
      "\n",
      " using head \n",
      "      0   1         2         3         4         5     6     7  8\n",
      "0 -315  75  0.205128  0.230769  0.307692  0.146667  0.28  0.24  0\n"
     ]
    }
   ],
   "source": [
    "print(\"using iloc \\n\", df.iloc[0:1])\n",
    "\n",
    "print(\"\\n\\n using head \\n\", df.head(1))\n",
    "\n",
    "#pulling out values\n",
    "sep = list(df.head(1).values[0])\n",
    "# n0 = sep[0]\n",
    "# n1 = sep[1]\n",
    "\n",
    "n0 = 0\n",
    "n1 = 0\n",
    "\n",
    "for sample in df[8]:\n",
    "    if sample == 0:\n",
    "        n0 = n0+1 \n",
    "    elif sample == 1:\n",
    "        n1 = n1+1\n",
    "\n",
    "    #print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using [n0,n1] format\n",
      "[40060 , 19940]\n"
     ]
    }
   ],
   "source": [
    "print(\"Using [n0,n1] format\")\n",
    "print(\"[\"+ str(n0) +\" , \" + str(n1) +\"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : C\n",
    "Print standard deviation of the second feature: \"The length of shorter sequence\" of the validation dataset: \"dataset/validation.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The standard deviation of column 'The length of shorter sequence' is :\n",
      "21.278652371558504\n"
     ]
    }
   ],
   "source": [
    "#print(\"The standard deviation of column 'The length of shorter sequence' is :\")\n",
    "#print(df['The length of shorter sequence'].std())\n",
    "\n",
    "print(\"The standard deviation of column 'The length of shorter sequence' is :\")\n",
    "print(df.iloc[:,1].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another way\n",
      "21.278652371558504\n"
     ]
    }
   ],
   "source": [
    "print(\"Another way\")\n",
    "print(df[1].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : D\n",
    "Print median (i.e., 50% percentile) of the seventh feature: \"'U' frequencies of sequence 2\" of the validation dataset: \"dataset/validation.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The median of column '7' is :\n",
      "0.220339\n"
     ]
    }
   ],
   "source": [
    "print(\"The median of column '7' is :\")\n",
    "print(df[6].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using iloc for median: \n",
      " 0.220339\n"
     ]
    }
   ],
   "source": [
    "print(\"Using iloc for median: \\n\",df.iloc[:,6].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : E\n",
    "Complete the function \"confusion_matrix\" partially defined that takes two arrays of target variable \"y\": y_actual and y_pred denoting ground truth class labels and predicted class labels for the N samples when N is the length of both the arrays. The function should return a list of 4 metrics: TN, FP, FN, TP (in this order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "def confusion_matrix(y_actual, y_pred):\n",
    "    # the function takes two arrays of target variable \"y\": y_actual and y_pred\n",
    "    #    denoting ground truth class labels and predicted class labels for the N samples\n",
    "    #    when N is the length of both the arrays.\n",
    "    # The function should return a list of 4 metrics: TN, FP, FN, TP (in this order).\n",
    "    assert(len(y_actual)==len(y_pred))\n",
    "    \n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    #@TODO\n",
    "    \n",
    "    for i in range(len(y_actual)):\n",
    "        if y_pred[i] == 0 and y_actual[i] == 0:\n",
    "            TN = TN + 1\n",
    "        elif y_pred[i] == 0 and y_actual[i] == 1:\n",
    "            FN = FN + 1\n",
    "        elif y_pred[i] == 1 and y_actual[i] == 1:\n",
    "            TP = TP + 1\n",
    "        elif y_pred[i] == 1 and y_actual[i] == 0:\n",
    "            FP = FP + 1\n",
    "\n",
    "    return [TN,FP, FN, TP]\n",
    "\n",
    "print(confusion_matrix([1, 0, 1, 1], [0, 0, 1, 1])) #Expected to print: [1, 0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#messing around with built in libraries\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "#This can't be here since I'm confusing the confusion matrices \n",
    "\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# TN,FP, FN, TP = confusion_matrix([1, 0, 1, 1], [0, 0, 1, 1]).ravel() #Expected to print: [1, 0, 1, 2]\n",
    "# print(\"Output from sklearn\") \n",
    "# print(\"TN, FP, FN, TP =\", TN,FP, FN, TP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : F\n",
    "You need to complete the accuracy function partially defined in the file that takes a confusion matrix, i.e. the list of the four metrics: [TN, FP, FN, TP], in this order, and return accuracy. In case of Division by Zero error, return -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return accuracy\n",
    "    [TN, FP, FN, TP] = conf_mat\n",
    "    \n",
    "    acc_value = 0\n",
    "    \n",
    "    #@TODO\n",
    "    try:\n",
    "        acc_value = (TP + TN) / (TN + FP + FN + TP)\n",
    "    except ZeroDivisionError:\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "    return acc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix([1, 0, 1, 1], [0, 0, 1, 1])  #Expected to print 0.75\n",
    "print(accuracy(conf_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print(\"Using sklearn library\")\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# def accuracy(conf_mat):\n",
    "#     # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "#     # return accuracy\n",
    "#     [TN, FP, FN, TP] = conf_mat\n",
    "    \n",
    "#     acc_value = 0\n",
    "    \n",
    "#     #@TODO\n",
    "    \n",
    "#     try:\n",
    "#         acc_value = (TP + TN) / (TN + FP + FN + TP)\n",
    "#     except ZeroDivisionError:\n",
    "#         print(\"-1\")\n",
    "    \n",
    "    \n",
    "#     return acc_value\n",
    "\n",
    "# conf_mat = TN,FP, FN, TP \n",
    "# conf_mat = confusion_matrix([1, 0, 1, 1], [0, 0, 1, 1]).ravel()\n",
    "# print(\"Accuracy\", accuracy(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : G\n",
    "You need to complete the precision function partially defined in the file that takes a confusion matrix, i.e. the list of the four metrics: [TN, FP, FN, TP], in this order, and return precision. It is also known as Positive Predictive Value (PPV). In case of Division by Zero error, return -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return precision. It is also known as Positive Predictive Value (PPV)\n",
    "    [TN, FP, FN, TP] = conf_mat\n",
    "    prec_value = 0\n",
    "    \n",
    "    \n",
    "    #@TODO\n",
    "    try:\n",
    "        prec_value = TP/ (TP + FP)\n",
    "    except ZeroDivisionError:\n",
    "        return -1    \n",
    "    \n",
    "    \n",
    "    return prec_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix([1, 0, 1, 1], [0, 0, 1, 1])  #Expected to print 1.0\n",
    "print(precision(conf_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Using sklearn library\")\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# def precision(conf_mat):\n",
    "#     # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "#     # return precision. It is also known as Positive Predictive Value (PPV)\n",
    "#     [TN, FP, FN, TP] = conf_mat\n",
    "#     prec_value = 0\n",
    "    \n",
    "    \n",
    "#     #@TODO\n",
    "#     try:\n",
    "#         prec_value = TP / (TP + FP)\n",
    "#     except ZeroDivisionError:\n",
    "#         print(\"-1\")    \n",
    "    \n",
    "    \n",
    "#     return prec_value\n",
    "\n",
    "# conf_mat = TN, FP, FN, TP \n",
    "\n",
    "# conf_mat = confusion_matrix([1, 0, 1, 1], [0, 0, 1, 1]).ravel()\n",
    "# print(\"Precision: \",precision(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : H\n",
    "You need to complete the recall function partially defined in the file that takes a confusion matrix, i.e. the list of the four metrics: [TN, FP, FN, TP], in this order, and return recall. It is also known as Sensitivity, or True Positive Rate (TPR). In case of Division by Zero error, return -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return recall. It is also known as Sensitivity, or True Positive Rate (TPR)\n",
    "    [TN, FP, FN, TP] = conf_mat\n",
    "    rec_value = 0\n",
    "    \n",
    "    #@TODO\n",
    "    try:\n",
    "        rec_value = TP / (TP + FN)\n",
    "    except ZeroDivisionError:\n",
    "        return -1   \n",
    "    \n",
    "    return rec_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix([1, 0, 1, 1], [0, 0, 1, 1])  #Expected to print 0.6666666666\n",
    "print(recall(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : I\n",
    "You need to complete the F1 function partially defined in the file that takes a confusion matrix, i.e. the list of the four metrics: [TN, FP, FN, TP], in this order, and return F1. It is the harmonic mean of precision and recall. In case of Division by Zero error, return -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return F1 score. It is the harmonic mean of precision and recall\n",
    "    [TN, FP, FN, TP] = conf_mat\n",
    "    f1_value = 0\n",
    "    \n",
    "    #@TODO\n",
    "    try:\n",
    "        f1_value = (2 * TP)/ ((2* TP) + FN + FP)\n",
    "    except ZeroDivisionError:\n",
    "        return -1  \n",
    "    \n",
    "    \n",
    "    return f1_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix([1, 0, 1, 1], [0, 0, 1, 1]) #Expected to print 0.8\n",
    "print(F1(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : J\n",
    "You need to complete the MCC function defined in the file that takes a confusion matrix, i.e. the list of the four metrics: [TN, FP, FN, TP], in this order, and return Matthews Correlation Coefficient (MCC). In case of Division by Zero error, return -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "\n",
    "def MCC(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return Matthews correlation coefficient (MCC)\n",
    "    [TN, FP, FN, TP] = conf_mat\n",
    "    mcc_value = 0\n",
    "    \n",
    "    #@TODO\n",
    "    try:\n",
    "        mcc_value = (TP * TN - FP * FN) / (sqrt((TP + FP) * (TP + FN) * (TN + FP)* (TN + FN)))\n",
    "    except ZeroDivisionError:\n",
    "        return -1  \n",
    "    \n",
    "    return mcc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5773502691896258\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix([1, 0, 1, 1], [0, 0, 1, 1]) #Expected to print 0.5773502691896258\n",
    "print(MCC(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : K\n",
    "You need to complete the FDR function defined in the file that takes a confusion matrix, i.e. the list of the four metrics: [TN, FP, FN, TP], in this order, and return False Discovery Rate (FDR). In case of Division by Zero error, return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FDR(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return False Discovery Rate (FDR)\n",
    "    [TN, FP, FN, TP] = conf_mat\n",
    "    fdr_value = 0\n",
    "    \n",
    "    #@TODO\n",
    "    try:\n",
    "        #https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "        fdr_value = FP/ (FP + TP)\n",
    "    except ZeroDivisionError:\n",
    "        return -1\n",
    "    \n",
    "    return fdr_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix([1, 0, 1, 1], [0, 0, 1, 1]) #Expected to print 0.0\n",
    "print(FDR(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : L\n",
    "Print as a dataframe containing:\n",
    " {model_name,acc,prec,rec,f1,mcc,FDR} for each of the N models (listed in models/*) after predicting the target variables of the validation data: \"dataset/validation.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 17%|██████████████                                                                      | 1/6 [00:00<00:01,  2.85it/s]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator KNeighborsClassifier from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 33%|████████████████████████████                                                        | 2/6 [00:02<00:02,  1.34it/s]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator SVC from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 50%|██████████████████████████████████████████                                          | 3/6 [00:30<00:27,  9.16s/it]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator MLPClassifier from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 67%|████████████████████████████████████████████████████████                            | 4/6 [00:31<00:13,  6.64s/it]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 83%|██████████████████████████████████████████████████████████████████████              | 5/6 [00:31<00:04,  4.75s/it]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator SVC from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:35<00:00,  5.98s/it]\n"
     ]
    }
   ],
   "source": [
    "#Print as a dataframe containing:\n",
    "# {model_name,acc,prec,rec,f1,mcc,FDR} for each of the N models (listed in model_files) predicting the target variables\n",
    "#  of the validation data.\n",
    "from math import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "result = pd.DataFrame(columns=['model_name','Accuracy','Precision','Recall','F1','MCC','FDR'])\n",
    "\n",
    "model_files=[\"models/Model_1.pkl\",\"models/Model_2.pkl\",\"models/Model_3.pkl\",\\\n",
    "                            \"models/Model_4.pkl\",\"models/Model_5.pkl\",\"models/Model_6.pkl\"]\n",
    "\n",
    "\n",
    "for file_name in tqdm(model_files):\n",
    "    in_file = open(file_name,'rb')\n",
    "    model = pickle.load(in_file)\n",
    "    in_file.close()\n",
    "\n",
    "    #Do the prediction by calling the \"predict\" member function of the model object on the\n",
    "    # validation data with the 8 features\n",
    "    X_test = validation.iloc[:,:-1]\n",
    "    y_test = validation.iloc[:,-1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    #@TODO: Complete and revise the following...\n",
    "    \n",
    "    conf_mat = []\n",
    "    acc = 0\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    f1 = 0\n",
    "    mcc = 0\n",
    "    fdr = 0\n",
    "\n",
    "###########################################################\n",
    "    \n",
    "    def confusion_matrix(y_actual, y_pred):\n",
    "    # the function takes two arrays of target variable \"y\": y_actual and y_pred\n",
    "    #    denoting ground truth class labels and predicted class labels for the N samples\n",
    "    #    when N is the length of both the arrays.\n",
    "    # The function should return a list of 4 metrics: TN, FP, FN, TP (in this order).\n",
    "        assert(len(y_actual)==len(y_pred))\n",
    "\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "\n",
    "        #@TODO\n",
    "\n",
    "        for i in range(len(y_actual)):\n",
    "            if y_pred[i] == 0 and y_actual[i] == 0:\n",
    "                TN = TN + 1\n",
    "            elif y_pred[i] == 0 and y_actual[i] == 1:\n",
    "                FN = FN + 1\n",
    "            elif y_pred[i] == 1 and y_actual[i] == 1:\n",
    "                TP = TP + 1\n",
    "            elif y_pred[i] == 1 and y_actual[i] == 0:\n",
    "                FP = FP + 1\n",
    "\n",
    "        return [TN,FP, FN, TP]\n",
    "\n",
    "###########################################\n",
    "\n",
    "    def accuracy(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return accuracy\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "    \n",
    "        acc_value = 0\n",
    "    \n",
    "        #@TODO\n",
    "        try:\n",
    "            acc_value = (TP + TN) / (TN + FP + FN + TP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1\n",
    "    \n",
    "    \n",
    "        return acc_value\n",
    "\n",
    "############################################\n",
    "\n",
    "    def precision(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return precision. It is also known as Positive Predictive Value (PPV)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        prec_value = 0\n",
    "    \n",
    "    \n",
    "        #@TODO\n",
    "        try:\n",
    "            prec_value = TP/ (TP + FP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1   \n",
    "    \n",
    "    \n",
    "        return prec_value\n",
    "\n",
    "##########################################\n",
    "\n",
    "    def recall(conf_mat):\n",
    "        # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "        # return recall. It is also known as Sensitivity, or True Positive Rate (TPR)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        rec_value = 0\n",
    "\n",
    "        #@TODO\n",
    "        try:\n",
    "            rec_value = TP / (TP + FN)\n",
    "        except ZeroDivisionError:\n",
    "            return -1   \n",
    "\n",
    "        return rec_value\n",
    "\n",
    "##########################################\n",
    "\n",
    "    def F1(conf_mat):\n",
    "        # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "        # return F1 score. It is the harmonic mean of precision and recall\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        f1_value = 0\n",
    "\n",
    "        #@TODO\n",
    "        try:\n",
    "            f1_value = (2 * TP)/ ((2* TP) + FN + FP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1 \n",
    "\n",
    "\n",
    "        return f1_value\n",
    "    \n",
    "#############################################\n",
    "\n",
    "    def MCC(conf_mat):\n",
    "        # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "        # return Matthews correlation coefficient (MCC)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        mcc_value = 0\n",
    "\n",
    "        #@TODO\n",
    "        try:\n",
    "            mcc_value = (TP * TN - FP * FN) / (sqrt((TP + FP) * (TP + FN) * (TN + FP)* (TN + FN)))\n",
    "        except ZeroDivisionError:\n",
    "            return -1  \n",
    "\n",
    "        return mcc_value\n",
    "    \n",
    "#####################################################\n",
    "    \n",
    "    def FDR(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return False Discovery Rate (FDR)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        fdr_value = 0\n",
    "    \n",
    "        #@TODO\n",
    "        try:\n",
    "        #https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "            fdr_value = FP/ (FP + TP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1 \n",
    "    \n",
    "        return fdr_value\n",
    "    \n",
    "    \n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    acc = accuracy(conf_mat)\n",
    "    prec = precision(conf_mat)\n",
    "    rec = recall(conf_mat)\n",
    "    f1 = F1(conf_mat)\n",
    "    mcc = MCC(conf_mat)\n",
    "    fdr = FDR(conf_mat)\n",
    "    \n",
    "    \n",
    "    result = result.append(pd.Series([file_name,acc,prec,rec,f1,mcc,fdr],index=result.columns),ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           model_name  Accuracy  Precision   Recall        F1       MCC  \\\n",
      "0  models/Model_1.pkl  0.337583   0.334082  0.99995  0.500835  0.050929   \n",
      "1  models/Model_2.pkl  0.334083   0.332910  0.99995  0.499518  0.028982   \n",
      "2  models/Model_3.pkl  0.667667  -1.000000  0.00000  0.000000 -1.000000   \n",
      "3  models/Model_4.pkl  0.390483   0.352497  0.99659  0.520789  0.168805   \n",
      "4  models/Model_5.pkl  0.333583   0.332744  0.99995  0.499330  0.024302   \n",
      "5  models/Model_6.pkl  0.337500   0.334054  0.99995  0.500804  0.050516   \n",
      "\n",
      "        FDR  \n",
      "0  0.665918  \n",
      "1  0.667090  \n",
      "2 -1.000000  \n",
      "3  0.647503  \n",
      "4  0.667256  \n",
      "5  0.665946  \n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : M\n",
    "Print the model name with path which is performing superior among the 6 pretrained models in terms of accuracy, given the performance result dataframe from “L”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name    models/Model_3.pkl\n",
       "Accuracy                0.667667\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = result\n",
    "#https://stackoverflow.com/questions/15741759/find-maximum-value-of-a-column-and-return-the-corresponding-row-values-using-pan\n",
    "\n",
    "result.iloc[result.Accuracy.argmax(), 0:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : N\n",
    "Print the model name with path which is performing the worst among the 6 pretrained models in terms of recall, given the performance result dataframe from “L”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/Model_3.pkl \n",
      "Recall      0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "low = result.iloc[result.Recall.argmin(), 0:4]\n",
    "print(low[0], \"\\nRecall     \", low[3])\n",
    "# print(\"\\n\\nUsing Min funciton\")\n",
    "# data['Recall'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : O\n",
    "Scale all the features of the validation set using the formula, z = (x-m)/s,\n",
    "    \n",
    "    where m = mean of a feature in the training set: \"dataset/training.csv\"\n",
    "    \n",
    "    s = standard deviation of the feature in the training set: \"dataset/training.csv\"\n",
    "    \n",
    "    #  DO NOT SCALE the target feature.\n",
    "    \n",
    "    # At the end, return a tuple (X, y), with X being a numpy array of shape (N,8) and y is an N dim array and \n",
    "\n",
    "N is the total number of samples in the validation set: \"dataset/validation.csv\".\n",
    "\n",
    "Store the scaled dataset in a variable (preferably a dataframe) named “X_validation_scaled”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0         1         2         3         4         5         6  \\\n",
      "0      0.775328 -1.002997 -0.325953  0.167255  1.069653 -1.676320  1.387452   \n",
      "1      1.155149 -1.002997 -0.163911 -0.761209  0.723327  3.906838  0.758204   \n",
      "2      0.912485 -0.910311 -0.579139  0.167255  0.451215 -0.042980 -0.009957   \n",
      "3     -1.656587  0.989744  0.978644 -0.409145  0.189161 -0.500924 -0.500269   \n",
      "4      0.390230 -0.910311  0.090006  0.321487 -0.321826  1.101893  1.215834   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "59995 -1.292591  1.082430  0.231075 -0.129629  0.080167  0.233696  0.482927   \n",
      "59996  0.305826  0.943401  0.518031  0.347192 -1.404098  0.091722 -0.064040   \n",
      "59997 -2.031134  1.082430  0.231075 -0.305082  0.281151  0.233696 -0.303639   \n",
      "59998  1.376712 -1.002997 -0.427228  0.080897  0.723327 -0.329198 -0.929301   \n",
      "59999 -1.624935  1.082430  0.317418 -2.102599  0.765511 -0.317275 -0.106986   \n",
      "\n",
      "              7  8  \n",
      "0     -0.658364  0  \n",
      "1     -2.059975  0  \n",
      "2      0.201713  0  \n",
      "3      0.699464  1  \n",
      "4     -1.504802  1  \n",
      "...         ... ..  \n",
      "59995 -0.395560  1  \n",
      "59996 -0.340345  0  \n",
      "59997 -0.176566  1  \n",
      "59998  0.884320  0  \n",
      "59999  0.042454  1  \n",
      "\n",
      "[60000 rows x 9 columns]\n",
      "Shape  (60000, 9)\n"
     ]
    }
   ],
   "source": [
    "#scale the individual columns in training\n",
    "formula_df = pd.DataFrame(validation)\n",
    "\n",
    "for feature in training.columns:\n",
    "    if feature == 8:\n",
    "        break;\n",
    "    #find the means in columns in training\n",
    "    mean = training.mean()\n",
    "    #print(\"Mean of columns in training set\\n\",mean)\n",
    "\n",
    "\n",
    "    #find the std in training\n",
    "    std = training.std()\n",
    "    #print(\"\\n\\nStandard deviation of training set\\n\",std)\n",
    "    \n",
    "for i in validation:\n",
    "    if i == 8:\n",
    "        break;\n",
    "    formula_df[i] = (validation[i]-mean[i])/std[i]\n",
    "\n",
    "print(formula_df)\n",
    "\n",
    "\n",
    "print(\"Shape \", formula_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : P\n",
    "Print as a dataframe containing:\n",
    "  {model_name,acc,prec,rec,f1,mcc,FDR} for each of the N models (listed in model_files) after  predicting the target variables \"y\" (given) for \"X\" (the scaled validation dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 17%|██████████████                                                                      | 1/6 [00:00<00:01,  4.83it/s]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator KNeighborsClassifier from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 33%|████████████████████████████                                                        | 2/6 [00:03<00:04,  1.09s/it]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator SVC from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 50%|██████████████████████████████████████████                                          | 3/6 [00:10<00:08,  2.84s/it]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator MLPClassifier from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 67%|████████████████████████████████████████████████████████                            | 4/6 [00:10<00:04,  2.17s/it]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 83%|██████████████████████████████████████████████████████████████████████              | 5/6 [00:11<00:01,  1.59s/it]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator SVC from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:15<00:00,  2.50s/it]\n"
     ]
    }
   ],
   "source": [
    "#@TODO\n",
    "\n",
    "#Print as a dataframe containing:\n",
    "# {model_name,acc,prec,rec,f1,mcc,FDR} for each of the N models (listed in model_files) predicting the target variables\n",
    "#  of the validation data.\n",
    "from math import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "result = pd.DataFrame(columns=['model_name','Accuracy','Precision','Recall','F1','MCC','FDR'])\n",
    "\n",
    "model_files=[\"models/Model_1.pkl\",\"models/Model_2.pkl\",\"models/Model_3.pkl\",\\\n",
    "                            \"models/Model_4.pkl\",\"models/Model_5.pkl\",\"models/Model_6.pkl\"]\n",
    "\n",
    "\n",
    "for file_name in tqdm(model_files):\n",
    "    in_file = open(file_name,'rb')\n",
    "    model = pickle.load(in_file)\n",
    "    in_file.close()\n",
    "\n",
    "    #Do the prediction by calling the \"predict\" member function of the model object on the\n",
    "    # validation data with the 8 features\n",
    "    X_test = formula_df.iloc[:,:-1]\n",
    "    y_test = formula_df.iloc[:,-1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    #@TODO: Complete and revise the following...\n",
    "    \n",
    "    conf_mat = []\n",
    "    acc = 0\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    f1 = 0\n",
    "    mcc = 0\n",
    "    fdr = 0\n",
    "\n",
    "###########################################################\n",
    "    \n",
    "    def confusion_matrix(y_actual, y_pred):\n",
    "    # the function takes two arrays of target variable \"y\": y_actual and y_pred\n",
    "    #    denoting ground truth class labels and predicted class labels for the N samples\n",
    "    #    when N is the length of both the arrays.\n",
    "    # The function should return a list of 4 metrics: TN, FP, FN, TP (in this order).\n",
    "        assert(len(y_actual)==len(y_pred))\n",
    "\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "\n",
    "        #@TODO\n",
    "\n",
    "        for i in range(len(y_actual)):\n",
    "            if y_pred[i] == 0 and y_actual[i] == 0:\n",
    "                TN = TN + 1\n",
    "            elif y_pred[i] == 0 and y_actual[i] == 1:\n",
    "                FN = FN + 1\n",
    "            elif y_pred[i] == 1 and y_actual[i] == 1:\n",
    "                TP = TP + 1\n",
    "            elif y_pred[i] == 1 and y_actual[i] == 0:\n",
    "                FP = FP + 1\n",
    "\n",
    "        return [TN,FP, FN, TP]\n",
    "\n",
    "###########################################\n",
    "\n",
    "    def accuracy(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return accuracy\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "    \n",
    "        acc_value = 0\n",
    "    \n",
    "        #@TODO\n",
    "        try:\n",
    "            acc_value = (TP + TN) / (TN + FP + FN + TP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1\n",
    "    \n",
    "    \n",
    "        return acc_value\n",
    "\n",
    "############################################\n",
    "\n",
    "    def precision(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return precision. It is also known as Positive Predictive Value (PPV)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        prec_value = 0\n",
    "    \n",
    "    \n",
    "        #@TODO\n",
    "        try:\n",
    "            prec_value = TP/ (TP + FP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1   \n",
    "    \n",
    "    \n",
    "        return prec_value\n",
    "\n",
    "##########################################\n",
    "\n",
    "    def recall(conf_mat):\n",
    "        # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "        # return recall. It is also known as Sensitivity, or True Positive Rate (TPR)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        rec_value = 0\n",
    "\n",
    "        #@TODO\n",
    "        try:\n",
    "            rec_value = TP / (TP + FN)\n",
    "        except ZeroDivisionError:\n",
    "            return -1  \n",
    "\n",
    "        return rec_value\n",
    "\n",
    "##########################################\n",
    "\n",
    "    def F1(conf_mat):\n",
    "        # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "        # return F1 score. It is the harmonic mean of precision and recall\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        f1_value = 0\n",
    "\n",
    "        #@TODO\n",
    "        try:\n",
    "            f1_value = (2 * TP)/ ((2* TP) + FN + FP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1 \n",
    "\n",
    "\n",
    "        return f1_value\n",
    "    \n",
    "#############################################\n",
    "\n",
    "    def MCC(conf_mat):\n",
    "        # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "        # return Matthews correlation coefficient (MCC)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        mcc_value = 0\n",
    "\n",
    "        #@TODO\n",
    "        try:\n",
    "            mcc_value = (TP * TN - FP * FN) / (sqrt((TP + FP) * (TP + FN) * (TN + FP)* (TN + FN)))\n",
    "        except ZeroDivisionError:\n",
    "            return -1 \n",
    "\n",
    "        return mcc_value\n",
    "    \n",
    "#####################################################\n",
    "    \n",
    "    def FDR(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return False Discovery Rate (FDR)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        fdr_value = 0\n",
    "    \n",
    "        #@TODO\n",
    "        try:\n",
    "        #https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "            fdr_value = FP/ (FP + TP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1\n",
    "    \n",
    "        return fdr_value\n",
    "    \n",
    "    \n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    acc = accuracy(conf_mat)\n",
    "    prec = precision(conf_mat)\n",
    "    rec = recall(conf_mat)\n",
    "    f1 = F1(conf_mat)\n",
    "    mcc = MCC(conf_mat)\n",
    "    fdr = FDR(conf_mat)\n",
    "    \n",
    "    \n",
    "    result = result.append(pd.Series([file_name,acc,prec,rec,f1,mcc,fdr],index=result.columns),ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           model_name  Accuracy  Precision    Recall        F1       MCC  \\\n",
      "0  models/Model_1.pkl  0.951233   0.944276  0.906770  0.925143  0.889404   \n",
      "1  models/Model_2.pkl  0.947700   0.936597  0.903811  0.919912  0.881411   \n",
      "2  models/Model_3.pkl  0.965283   0.946492  0.949198  0.947843  0.921828   \n",
      "3  models/Model_4.pkl  0.966100   0.948907  0.949097  0.949002  0.923614   \n",
      "4  models/Model_5.pkl  0.914133   0.887770  0.848947  0.867925  0.804802   \n",
      "5  models/Model_6.pkl  0.953883   0.936816  0.923521  0.930121  0.895760   \n",
      "\n",
      "        FDR  \n",
      "0  0.055724  \n",
      "1  0.063403  \n",
      "2  0.053508  \n",
      "3  0.051093  \n",
      "4  0.112230  \n",
      "5  0.063184  \n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : Q\n",
    "Print the model name with path which is performing superior in terms of accuracy, given the performance result dataframe from “P”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name    models/Model_4.pkl\n",
       "Accuracy                  0.9661\n",
       "Name: 3, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.iloc[result.Accuracy.argmax(), 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : R\n",
    "Print the model name with path which is performing the worst in terms of recall, given the performance result dataframe from “P”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/Model_5.pkl \n",
      "Recall      0.8489468405215647\n"
     ]
    }
   ],
   "source": [
    "low = result.iloc[result.Recall.argmin(), 0:4]\n",
    "print(low[0], \"\\nRecall     \", low[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : S\n",
    "Flip the prediction of Model 1, and then compute and\n",
    " print as a dataframe containing: {acc,prec,rec,f1,mcc,FDR} \n",
    "on the original (i.e., not-scaled) validation dataset: \"dataset/validation.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flip the prediction of model 1 on last column \n",
    "\n",
    "#initialize dataframe \n",
    "flipped = pd.DataFrame(validation)\n",
    "#print(\"Preflip: \\n\",flipped.iloc[: , -1] )\n",
    "\n",
    "#one line this ish\n",
    "flipped = flipped[:-1].replace({0:1, 1:0})\n",
    "        \n",
    "#print(\"\\n\\npostflip: \\n\",flipped.iloc[: , -1] )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           model_name  Accuracy  Precision    Recall        F1     MCC  \\\n",
      "0  models/Model_1.pkl  0.048767   0.055727  0.026635  0.036043 -0.8894   \n",
      "\n",
      "        FDR  \n",
      "0  0.944273  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#@TODO\n",
    "\n",
    "#Print as a dataframe containing:\n",
    "# {model_name,acc,prec,rec,f1,mcc,FDR} for each of the N models (listed in model_files) predicting the target variables\n",
    "#  of the validation data.\n",
    "from math import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "result = pd.DataFrame(columns=['model_name','Accuracy','Precision','Recall','F1','MCC','FDR'])\n",
    "\n",
    "model_files=[\"models/Model_1.pkl\"]\n",
    "\n",
    "\n",
    "for file_name in tqdm(model_files):\n",
    "    in_file = open(file_name,'rb')\n",
    "    model = pickle.load(in_file)\n",
    "    in_file.close()\n",
    "\n",
    "    #Do the prediction by calling the \"predict\" member function of the model object on the\n",
    "    # validation data with the 8 features\n",
    "    X_test = flipped.iloc[:,:-1]\n",
    "    y_test = flipped.iloc[:,-1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    #@TODO: Complete and revise the following...\n",
    "    \n",
    "    conf_mat = []\n",
    "    acc = 0\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    f1 = 0\n",
    "    mcc = 0\n",
    "    fdr = 0\n",
    "\n",
    "###########################################################\n",
    "    \n",
    "    def confusion_matrix(y_actual, y_pred):\n",
    "    # the function takes two arrays of target variable \"y\": y_actual and y_pred\n",
    "    #    denoting ground truth class labels and predicted class labels for the N samples\n",
    "    #    when N is the length of both the arrays.\n",
    "    # The function should return a list of 4 metrics: TN, FP, FN, TP (in this order).\n",
    "        assert(len(y_actual)==len(y_pred))\n",
    "\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "\n",
    "        #@TODO\n",
    "\n",
    "        for i in range(len(y_actual)):\n",
    "            if y_pred[i] == 0 and y_actual[i] == 0:\n",
    "                TN = TN + 1\n",
    "            elif y_pred[i] == 0 and y_actual[i] == 1:\n",
    "                FN = FN + 1\n",
    "            elif y_pred[i] == 1 and y_actual[i] == 1:\n",
    "                TP = TP + 1\n",
    "            elif y_pred[i] == 1 and y_actual[i] == 0:\n",
    "                FP = FP + 1\n",
    "\n",
    "        return [TN,FP, FN, TP]\n",
    "\n",
    "###########################################\n",
    "\n",
    "    def accuracy(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return accuracy\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "    \n",
    "        acc_value = 0\n",
    "    \n",
    "        #@TODO\n",
    "        try:\n",
    "            acc_value = (TP + TN) / (TN + FP + FN + TP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1\n",
    "    \n",
    "    \n",
    "        return acc_value\n",
    "\n",
    "############################################\n",
    "\n",
    "    def precision(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return precision. It is also known as Positive Predictive Value (PPV)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        prec_value = 0\n",
    "    \n",
    "    \n",
    "        #@TODO\n",
    "        try:\n",
    "            prec_value = TP/ (TP + FP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1    \n",
    "    \n",
    "    \n",
    "        return prec_value\n",
    "\n",
    "##########################################\n",
    "\n",
    "    def recall(conf_mat):\n",
    "        # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "        # return recall. It is also known as Sensitivity, or True Positive Rate (TPR)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        rec_value = 0\n",
    "\n",
    "        #@TODO\n",
    "        try:\n",
    "            rec_value = TP / (TP + FN)\n",
    "        except ZeroDivisionError:\n",
    "            return -1   \n",
    "\n",
    "        return rec_value\n",
    "\n",
    "##########################################\n",
    "\n",
    "    def F1(conf_mat):\n",
    "        # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "        # return F1 score. It is the harmonic mean of precision and recall\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        f1_value = 0\n",
    "\n",
    "        #@TODO\n",
    "        try:\n",
    "            f1_value = (2 * TP)/ ((2* TP) + FN + FP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1 \n",
    "\n",
    "\n",
    "        return f1_value\n",
    "    \n",
    "#############################################\n",
    "\n",
    "    def MCC(conf_mat):\n",
    "        # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "        # return Matthews correlation coefficient (MCC)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        mcc_value = 0\n",
    "\n",
    "        #@TODO\n",
    "        try:\n",
    "            mcc_value = (TP * TN - FP * FN) / (sqrt((TP + FP) * (TP + FN) * (TN + FP)* (TN + FN)))\n",
    "        except ZeroDivisionError:\n",
    "            return -1  \n",
    "\n",
    "        return mcc_value\n",
    "    \n",
    "#####################################################\n",
    "    \n",
    "    def FDR(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return False Discovery Rate (FDR)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        fdr_value = 0\n",
    "    \n",
    "        #@TODO\n",
    "        try:\n",
    "        #https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "            fdr_value = FP/ (FP + TP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1 \n",
    "    \n",
    "        return fdr_value\n",
    "    \n",
    "    \n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    acc = accuracy(conf_mat)\n",
    "    prec = precision(conf_mat)\n",
    "    rec = recall(conf_mat)\n",
    "    f1 = F1(conf_mat)\n",
    "    mcc = MCC(conf_mat)\n",
    "    fdr = FDR(conf_mat)\n",
    "    \n",
    "    \n",
    "    result = result.append(pd.Series([file_name,acc,prec,rec,f1,mcc,fdr],index=result.columns),ignore_index=True)\n",
    "\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task : T\n",
    "Say, in a confusion matrix, the values of the four metrics are: TP=90, TN=1, FP=4, FN=5. Compute F1_original and MCC_original denoting the F1 and MCC scores.\n",
    "\n",
    "Now, flip the predictions, i.e., positives are now will be predicted as negative, and negatives are going to be predicted as positive. \n",
    "\n",
    "Then, compute F1_flipped and MCC_flipped, denoting corresponding F1 and MCC scores. \n",
    "\n",
    "Print/Return the new {TP, TN, FP, FN, F1_original,MCC_original,F1_flipped, MCC_flipped, COMMENT_string} as a dataframe, where COMMENT_string is a string that will be no longer than 200 characters but is going to be your comment about the F1 and MCC values for the two cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original [1, 4, 5, 90]\n",
      "flipped value [4, 1, 90, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-86ff993ed636>:3: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)  # or 199\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from math import *\n",
    "pd.set_option('display.max_colwidth', -1)  # or 199\n",
    "\n",
    "TP=90 \n",
    "TN=1 \n",
    "FP=4\n",
    "FN=5\n",
    "conf_mat = [TN, FP, FN, TP]\n",
    "print(\"original\", conf_mat)\n",
    "\n",
    "#flipping based on the confusion matrix \n",
    "newTP = FN\n",
    "newTN = FP\n",
    "newFP = TN\n",
    "newFN = TP\n",
    "flippedMat = [newTN, newFP, newFN, newTP]\n",
    "print(\"flipped value\", flippedMat)\n",
    "\n",
    "############################################\n",
    "\n",
    "def F1_og(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return F1 score. It is the harmonic mean of precision and recall\n",
    "    [TN, FP, FN, TP] = conf_mat\n",
    "    f1_value = 0\n",
    "    \n",
    "    #@TODO\n",
    "    try:\n",
    "        f1_value = (2 * TP)/ ((2* TP) + FN + FP)\n",
    "    except ZeroDivisionError:\n",
    "        return -1 \n",
    "    \n",
    "    \n",
    "    return f1_value\n",
    "\n",
    "##############################################\n",
    "\n",
    "def MCC_og(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return Matthews correlation coefficient (MCC)\n",
    "    [TN, FP, FN, TP] = conf_mat\n",
    "    mcc_value = 0\n",
    "    \n",
    "    #@TODO\n",
    "    try:\n",
    "        mcc_value = (TP * TN - FP * FN) / (sqrt((TP + FP) * (TP + FN) * (TN + FP)* (TN + FN)))\n",
    "    except ZeroDivisionError:\n",
    "        return -1  \n",
    "    \n",
    "    return mcc_value\n",
    "\n",
    "def F1_flip(flippedMat):\n",
    "    \n",
    "    flippedMat = [newTN, newFP, newFN, newTP]\n",
    "    f1_value = 0\n",
    "    \n",
    "    #@TODO\n",
    "    try:\n",
    "        f1_value = (2 * newTN)/ ((2* newTP) + newFP + newFN)\n",
    "    except ZeroDivisionError:\n",
    "        return -1 \n",
    "    \n",
    "    \n",
    "    return f1_value\n",
    "    \n",
    "    \n",
    "def MCC_flip(flippedMat): \n",
    "    flippedMat = [newTN, newFP, newFN, newTP]\n",
    "    mcc_value = 0\n",
    "    \n",
    "    #@TODO\n",
    "    try:\n",
    "        mcc_value = ((newTN * newTP) - (newFN * newFP))/ (sqrt((newTN + newFN) * (newTN + newFP) * (newTP + newFN)* (newTP + FP)))\n",
    "    except ZeroDivisionError:\n",
    "        return -1\n",
    "    \n",
    "    return mcc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG F1 0.9523809523809523\n",
      "\n",
      "\n",
      "OG MCC 0.13524203070138519\n",
      "\n",
      "\n",
      "Flipped F1 0.07920792079207921\n",
      "\n",
      "\n",
      "Flipped MCC -0.11042465566540356\n"
     ]
    }
   ],
   "source": [
    "print(\"OG F1\", F1_og(conf_mat))\n",
    "\n",
    "print(\"\\n\\nOG MCC\", MCC_og(conf_mat))\n",
    "\n",
    "print(\"\\n\\nFlipped F1\", F1_flip(flippedMat))\n",
    "\n",
    "print(\"\\n\\nFlipped MCC\", MCC_flip(flippedMat))\n",
    "\n",
    "F1_original = F1_og(conf_mat)\n",
    "MCC_original = MCC_og(conf_mat)\n",
    "F1_flipped = F1_flip(flippedMat)\n",
    "MCC_flipped = MCC_flip(flippedMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 1 4 5 0.9523809523809523 0.13524203070138519 0.07920792079207921 -0.11042465566540356 MCC was flipped from positive to negative and F1 are similar\n"
     ]
    }
   ],
   "source": [
    "print(TP, TN, FP, FN, F1_original,MCC_original,F1_flipped, MCC_flipped, \"MCC was flipped from positive to negative and F1 are similar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Graduate Students only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: U\n",
    "This task is a follow up of Task P. There is always cost associated with misclassifications. For instance, if a model predicts a ncRNA (class=1) to be non ncRNA (class=0), further verfication will then follow that includes going through the next generation sequencing of those samples costing USD 20 per sample. On the other hand, cost of predicting a non ncRNA to be ncRNA insignificant as most researchers do not care for ncRNAs. They might put it in a piece of paper as a note costing USD 1 per 5 samples. The same cost applies to correct predictions too.\n",
    "\n",
    "What is the cost of predictions with each of the six models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 17%|██████████████                                                                      | 1/6 [00:00<00:01,  4.80it/s]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator KNeighborsClassifier from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 33%|████████████████████████████                                                        | 2/6 [00:03<00:04,  1.12s/it]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator SVC from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 50%|██████████████████████████████████████████                                          | 3/6 [00:10<00:08,  2.86s/it]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator MLPClassifier from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 67%|████████████████████████████████████████████████████████                            | 4/6 [00:11<00:04,  2.20s/it]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 83%|██████████████████████████████████████████████████████████████████████              | 5/6 [00:11<00:01,  1.61s/it]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator SVC from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:15<00:00,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           model_name  Accuracy  Precision    Recall        F1       MCC  \\\n",
      "0  models/Model_1.pkl  0.951233  0.944276   0.906770  0.925143  0.889404   \n",
      "1  models/Model_2.pkl  0.947700  0.936597   0.903811  0.919912  0.881411   \n",
      "2  models/Model_3.pkl  0.965283  0.946492   0.949198  0.947843  0.921828   \n",
      "3  models/Model_4.pkl  0.966100  0.948907   0.949097  0.949002  0.923614   \n",
      "4  models/Model_5.pkl  0.914133  0.887770   0.848947  0.867925  0.804802   \n",
      "5  models/Model_6.pkl  0.953883  0.936816   0.923521  0.930121  0.895760   \n",
      "\n",
      "        FDR      Cost  \n",
      "0  0.055724  69934.8   \n",
      "1  0.063403  74132.4   \n",
      "2  0.053508  53243.4   \n",
      "3  0.051093  52273.2   \n",
      "4  0.112230  114009.6  \n",
      "5  0.063184  66786.6   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#@TODO\n",
    "\n",
    "#Print as a dataframe containing:\n",
    "# {model_name,acc,prec,rec,f1,mcc,FDR} for each of the N models (listed in model_files) predicting the target variables\n",
    "#  of the validation data.\n",
    "from math import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "result = pd.DataFrame(columns=['model_name','Accuracy','Precision','Recall','F1','MCC','FDR', \"Cost\"])\n",
    "\n",
    "model_files=[\"models/Model_1.pkl\",\"models/Model_2.pkl\",\"models/Model_3.pkl\",\\\n",
    "                            \"models/Model_4.pkl\",\"models/Model_5.pkl\",\"models/Model_6.pkl\"]\n",
    "\n",
    "\n",
    "for file_name in tqdm(model_files):\n",
    "    in_file = open(file_name,'rb')\n",
    "    model = pickle.load(in_file)\n",
    "    in_file.close()\n",
    "\n",
    "    #Do the prediction by calling the \"predict\" member function of the model object on the\n",
    "    # validation data with the 8 features\n",
    "    X_test = formula_df.iloc[:,:-1]\n",
    "    y_test = formula_df.iloc[:,-1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    #@TODO: Complete and revise the following...\n",
    "    \n",
    "    conf_mat = []\n",
    "    acc = 0\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    f1 = 0\n",
    "    mcc = 0\n",
    "    fdr = 0\n",
    "    cost = 0\n",
    "\n",
    "###########################################################\n",
    "    \n",
    "    def confusion_matrix(y_actual, y_pred):\n",
    "    # the function takes two arrays of target variable \"y\": y_actual and y_pred\n",
    "    #    denoting ground truth class labels and predicted class labels for the N samples\n",
    "    #    when N is the length of both the arrays.\n",
    "    # The function should return a list of 4 metrics: TN, FP, FN, TP (in this order).\n",
    "        assert(len(y_actual)==len(y_pred))\n",
    "\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "\n",
    "        #@TODO\n",
    "\n",
    "        for i in range(len(y_actual)):\n",
    "            if y_pred[i] == 0 and y_actual[i] == 0:\n",
    "                TN = TN + 1\n",
    "            elif y_pred[i] == 0 and y_actual[i] == 1:\n",
    "                FN = FN + 1\n",
    "            elif y_pred[i] == 1 and y_actual[i] == 1:\n",
    "                TP = TP + 1\n",
    "            elif y_pred[i] == 1 and y_actual[i] == 0:\n",
    "                FP = FP + 1\n",
    "\n",
    "        return [TN,FP, FN, TP]\n",
    "\n",
    "###########################################\n",
    "\n",
    "    def accuracy(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return accuracy\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "    \n",
    "        acc_value = 0\n",
    "    \n",
    "        #@TODO\n",
    "        try:\n",
    "            acc_value = (TP + TN) / (TN + FP + FN + TP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1\n",
    "    \n",
    "    \n",
    "        return acc_value\n",
    "\n",
    "############################################\n",
    "\n",
    "    def precision(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return precision. It is also known as Positive Predictive Value (PPV)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        prec_value = 0\n",
    "    \n",
    "    \n",
    "        #@TODO\n",
    "        try:\n",
    "            prec_value = TP/ (TP + FP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1    \n",
    "    \n",
    "    \n",
    "        return prec_value\n",
    "\n",
    "##########################################\n",
    "\n",
    "    def recall(conf_mat):\n",
    "        # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "        # return recall. It is also known as Sensitivity, or True Positive Rate (TPR)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        rec_value = 0\n",
    "\n",
    "        #@TODO\n",
    "        try:\n",
    "            rec_value = TP / (TP + FN)\n",
    "        except ZeroDivisionError:\n",
    "            return -1 \n",
    "\n",
    "        return rec_value\n",
    "\n",
    "##########################################\n",
    "\n",
    "    def F1(conf_mat):\n",
    "        # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "        # return F1 score. It is the harmonic mean of precision and recall\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        f1_value = 0\n",
    "\n",
    "        #@TODO\n",
    "        try:\n",
    "            f1_value = (2 * TP)/ ((2* TP) + FN + FP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1  \n",
    "\n",
    "\n",
    "        return f1_value\n",
    "    \n",
    "#############################################\n",
    "\n",
    "    def MCC(conf_mat):\n",
    "        # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "        # return Matthews correlation coefficient (MCC)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        mcc_value = 0\n",
    "\n",
    "        #@TODO\n",
    "        try:\n",
    "            mcc_value = (TP * TN - FP * FN) / (sqrt((TP + FP) * (TP + FN) * (TN + FP)* (TN + FN)))\n",
    "        except ZeroDivisionError:\n",
    "            return -1  \n",
    "\n",
    "        return mcc_value\n",
    "    \n",
    "#####################################################\n",
    "    \n",
    "    def FDR(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return False Discovery Rate (FDR)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        fdr_value = 0\n",
    "    \n",
    "        #@TODO\n",
    "        try:\n",
    "        #https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "            fdr_value = FP/ (FP + TP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1\n",
    "    \n",
    "        return fdr_value\n",
    "######################################################\n",
    "\n",
    "    def cost(conf_mat):\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "\n",
    "        FPcost = 20\n",
    "        FNcost = 20\n",
    "        TPcost = 0.2\n",
    "        TNcost = 0.2\n",
    "\n",
    "        TP = TP * TPcost\n",
    "        TN = TN * TNcost\n",
    "        FP = FP * FPcost\n",
    "        FN = FN * FNcost\n",
    "\n",
    "        total = TP + TN + FP + FN\n",
    "\n",
    "        return total\n",
    "#####################################################\n",
    "\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    acc = accuracy(conf_mat)\n",
    "    prec = precision(conf_mat)\n",
    "    rec = recall(conf_mat)\n",
    "    f1 = F1(conf_mat)\n",
    "    mcc = MCC(conf_mat)\n",
    "    fdr = FDR(conf_mat)\n",
    "    cost = cost(conf_mat)\n",
    "    \n",
    "    \n",
    "    result = result.append(pd.Series([file_name,acc,prec,rec,f1,mcc,fdr,cost],index=result.columns),ignore_index=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: V\n",
    "Please comment on which of the six models is the best on the cost basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best on cost basis would be  models/Model_4.pkl : 52273.2\n"
     ]
    }
   ],
   "source": [
    "#result.idxmin() \n",
    "low = result[\"Cost\"].min()\n",
    "\n",
    "lower = result.iloc[result.Cost.argmin(), 0:-1]\n",
    "print(\"The best on cost basis would be \", lower[0], \":\" , low)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: W\n",
    "Please comment on which of the six models is the best overall. Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off of the data, they are all pretty similar. Module 5 is the most costly so that is probably the worst overall on the basis alone. Going across the data, it looks like models 3 and 4 have similar values in accuracy, precision, recall, F1, MCC, FDR and cost. I would try to find the standard deviation of the two rows but I'm dying here and you cannot find the standard deviation of a row. The true answer would be subjective because there isn't a \"true\" model to compare the which one has the metrics that you absolutely need it to be the highest for. The varying difference is up to the thousandth of a degree which could be almost neglible to some people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: X\n",
    "Again a followup of Tasks O and P: Please scale the given validation dataset with an alternate scaling technique you can think of and repeat Task P with the modified scaled validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min max scaling = xnew = xi-xmin/max-min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0         1         2         3         4          5         6  \\\n",
      "0      0.831370  0.121212 -1.154189  0.190543  3.545302 -4.768564   4.093947   \n",
      "1      0.869914  0.121212 -0.720467 -2.779611  2.300441  10.175250  2.080984   \n",
      "2      0.845289  0.151515 -1.831862  0.190543  1.322343 -0.396784  -0.376359   \n",
      "3      0.584582  0.772727  2.337680 -1.653361  0.380396 -1.622513  -1.944865   \n",
      "4      0.792291  0.151515 -0.040838  0.683930 -1.456335  2.667567   3.544943   \n",
      "...         ...       ...       ...       ...       ...       ...        ...   \n",
      "59995  0.621520  0.803030  0.336746 -0.759190 -0.011381  0.343761   1.200375   \n",
      "59996  0.783726  0.757576  1.104811  0.766161 -5.346535 -0.036245  -0.549371   \n",
      "59997  0.546574  0.803030  0.336746 -1.320463  0.711053  0.343761  -1.315847   \n",
      "59998  0.892398  0.121212 -1.425258 -0.085716  2.300441 -1.162872  -3.317336   \n",
      "59999  0.587794  0.803030  0.567853 -7.070715  2.452072 -1.130959  -0.686754   \n",
      "\n",
      "              7  targets  \n",
      "0     -2.666013  0        \n",
      "1     -7.704069  0        \n",
      "2      0.425515  0        \n",
      "3      2.214665  1        \n",
      "4     -5.708514  1        \n",
      "...         ... ..        \n",
      "59995 -1.721372  1        \n",
      "59996 -1.522903  0        \n",
      "59997 -0.934202  1        \n",
      "59998  2.879126  0        \n",
      "59999 -0.146938  1        \n",
      "\n",
      "[60000 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "newScale = pd.DataFrame(validation)\n",
    "\n",
    "#cool removes last column\n",
    "newScale.drop(8, axis=1, inplace=True) \n",
    "\n",
    "\n",
    "minList = []\n",
    "maxList = []\n",
    "\n",
    "\n",
    "for feature in training.columns:\n",
    "    if feature== 8:\n",
    "        break;\n",
    "    \n",
    "    maxNum = training.loc[training[feature].argmax()][feature]\n",
    "    maxList.append(maxNum)\n",
    "                          \n",
    "    minNum = training.loc[training[feature].argmin()][feature]\n",
    "    minList.append(minNum) \n",
    "\n",
    "\n",
    "for feature in validation:\n",
    "    if feature == 8:\n",
    "        break;\n",
    "        \n",
    "    else: \n",
    "        newScale[feature] = (validation[feature] - minList[feature])/(maxList[feature] - minList[feature])\n",
    "        \n",
    "\n",
    "targets = validation[8]\n",
    "minMax = newScale\n",
    "\n",
    "#putting last column back in\n",
    "minMax['targets'] = targets\n",
    "\n",
    "print(minMax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 17%|██████████████                                                                      | 1/6 [00:00<00:01,  3.89it/s]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator KNeighborsClassifier from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 33%|████████████████████████████                                                        | 2/6 [00:06<00:07,  1.92s/it]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator SVC from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 50%|██████████████████████████████████████████                                          | 3/6 [00:13<00:10,  3.44s/it]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator MLPClassifier from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 67%|████████████████████████████████████████████████████████                            | 4/6 [00:13<00:05,  2.62s/it]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      " 83%|██████████████████████████████████████████████████████████████████████              | 5/6 [00:14<00:01,  1.91s/it]C:\\Users\\kerry\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator SVC from version 0.24.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:17<00:00,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           model_name  Accuracy  Precision    Recall        F1       MCC  \\\n",
      "0  models/Model_1.pkl  0.638850  0.346584   0.097944  0.152727  0.009740   \n",
      "1  models/Model_2.pkl  0.645583  0.368578   0.093180  0.148753  0.023303   \n",
      "2  models/Model_3.pkl  0.663833  0.406046   0.024925  0.046967  0.022582   \n",
      "3  models/Model_4.pkl  0.633733  0.352378   0.121866  0.181100  0.015335   \n",
      "4  models/Model_5.pkl  0.667667 -1.000000   0.000000  0.000000 -1.000000   \n",
      "5  models/Model_6.pkl  0.638333  0.346797   0.099900  0.155116  0.009990   \n",
      "\n",
      "        FDR  \n",
      "0  0.653416  \n",
      "1  0.631422  \n",
      "2  0.593954  \n",
      "3  0.647622  \n",
      "4 -1.000000  \n",
      "5  0.653203  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#@TODO\n",
    "\n",
    "#Print as a dataframe containing:\n",
    "# {model_name,acc,prec,rec,f1,mcc,FDR} for each of the N models (listed in model_files) predicting the target variables\n",
    "#  of the validation data.\n",
    "from math import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "result = pd.DataFrame(columns=['model_name','Accuracy','Precision','Recall','F1','MCC','FDR'])\n",
    "\n",
    "model_files=[\"models/Model_1.pkl\",\"models/Model_2.pkl\",\"models/Model_3.pkl\",\n",
    "                            \"models/Model_4.pkl\",\"models/Model_5.pkl\",\"models/Model_6.pkl\"]\n",
    "\n",
    "\n",
    "for file_name in tqdm(model_files):\n",
    "    in_file = open(file_name,'rb')\n",
    "    model = pickle.load(in_file)\n",
    "    in_file.close()\n",
    "\n",
    "    #Do the prediction by calling the \"predict\" member function of the model object on the\n",
    "    # validation data with the 8 features\n",
    "    X_test = newScale.iloc[:,:-1]\n",
    "    y_test = newScale.iloc[:,-1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    #@TODO: Complete and revise the following...\n",
    "    \n",
    "    conf_mat = []\n",
    "    acc = 0\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    f1 = 0\n",
    "    mcc = 0\n",
    "    fdr = 0\n",
    "\n",
    "###########################################################\n",
    "    \n",
    "    def confusion_matrix(y_actual, y_pred):\n",
    "    # the function takes two arrays of target variable \"y\": y_actual and y_pred\n",
    "    #    denoting ground truth class labels and predicted class labels for the N samples\n",
    "    #    when N is the length of both the arrays.\n",
    "    # The function should return a list of 4 metrics: TN, FP, FN, TP (in this order).\n",
    "        assert(len(y_actual)==len(y_pred))\n",
    "\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "\n",
    "        #@TODO\n",
    "\n",
    "        for i in range(len(y_actual)):\n",
    "            if y_pred[i] == 0 and y_actual[i] == 0:\n",
    "                TN = TN + 1\n",
    "            elif y_pred[i] == 0 and y_actual[i] == 1:\n",
    "                FN = FN + 1\n",
    "            elif y_pred[i] == 1 and y_actual[i] == 1:\n",
    "                TP = TP + 1\n",
    "            elif y_pred[i] == 1 and y_actual[i] == 0:\n",
    "                FP = FP + 1\n",
    "\n",
    "        return [TN,FP, FN, TP]\n",
    "\n",
    "###########################################\n",
    "\n",
    "    def accuracy(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return accuracy\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "    \n",
    "        acc_value = 0\n",
    "    \n",
    "        #@TODO\n",
    "        try:\n",
    "            acc_value = (TP + TN) / (TN + FP + FN + TP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1\n",
    "    \n",
    "    \n",
    "        return acc_value\n",
    "\n",
    "############################################\n",
    "\n",
    "    def precision(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return precision. It is also known as Positive Predictive Value (PPV)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        prec_value = 0\n",
    "    \n",
    "    \n",
    "        #@TODO\n",
    "        try:\n",
    "            prec_value = TP/ (TP + FP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1   \n",
    "    \n",
    "    \n",
    "        return prec_value\n",
    "\n",
    "##########################################\n",
    "\n",
    "    def recall(conf_mat):\n",
    "        # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "        # return recall. It is also known as Sensitivity, or True Positive Rate (TPR)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        rec_value = 0\n",
    "\n",
    "        #@TODO\n",
    "        try:\n",
    "            rec_value = TP / (TP + FN)\n",
    "        except ZeroDivisionError:\n",
    "            return -1  \n",
    "\n",
    "        return rec_value\n",
    "\n",
    "##########################################\n",
    "\n",
    "    def F1(conf_mat):\n",
    "        # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "        # return F1 score. It is the harmonic mean of precision and recall\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        f1_value = 0\n",
    "\n",
    "        #@TODO\n",
    "        try:\n",
    "            f1_value = (2 * TP)/ ((2* TP) + FN + FP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1  \n",
    "\n",
    "\n",
    "        return f1_value\n",
    "    \n",
    "#############################################\n",
    "\n",
    "    def MCC(conf_mat):\n",
    "        # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "        # return Matthews correlation coefficient (MCC)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        mcc_value = 0\n",
    "\n",
    "        #@TODO\n",
    "        try:\n",
    "            mcc_value = (TP * TN - FP * FN) / (sqrt((TP + FP) * (TP + FN) * (TN + FP)* (TN + FN)))\n",
    "        except ZeroDivisionError:\n",
    "            return -1 \n",
    "\n",
    "        return mcc_value\n",
    "    \n",
    "#####################################################\n",
    "    \n",
    "    def FDR(conf_mat):\n",
    "    # Given a confusion matrix, i.e. the list of four metrics: [TN,FP, FN, TP], in this order\n",
    "    # return False Discovery Rate (FDR)\n",
    "        [TN, FP, FN, TP] = conf_mat\n",
    "        fdr_value = 0\n",
    "    \n",
    "        #@TODO\n",
    "        try:\n",
    "        #https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "            fdr_value = FP/ (FP + TP)\n",
    "        except ZeroDivisionError:\n",
    "            return -1\n",
    "    \n",
    "        return fdr_value\n",
    "    \n",
    "    \n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    acc = accuracy(conf_mat)\n",
    "    prec = precision(conf_mat)\n",
    "    rec = recall(conf_mat)\n",
    "    f1 = F1(conf_mat)\n",
    "    mcc = MCC(conf_mat)\n",
    "    fdr = FDR(conf_mat)\n",
    "    \n",
    "    \n",
    "    result = result.append(pd.Series([file_name,acc,prec,rec,f1,mcc,fdr],index=result.columns),ignore_index=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Y\n",
    "Print the model name with path which is performing superior in terms of accuracy, given the performance result dataframe from “X”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name    models/Model_5.pkl\n",
       "Accuracy      0.667667          \n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.iloc[result.Accuracy.argmax(), 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Z\n",
    "Print the model name with path which is performing the worst in terms of recall, given the performance result dataframe from “X”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/Model_5.pkl : 0.0\n"
     ]
    }
   ],
   "source": [
    "low = result.iloc[result.Recall.argmin(), 0:4]\n",
    "print(low[0], \":\", low[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
